# import pandas as pd
# import matplotlib.pyplot as plt
# import uuid
# import os
# import openai
# import re
# import traceback
# from model import CustomModel
# from dotenv import load_dotenv
# import random as rt

# load_dotenv()
# openai.api_key = os.getenv("OPENAI_API_KEY")

# custom_model = CustomModel()

# def extract_code_block(text):
#     match = re.search(r"```(?:python)?(.*?)```", text, re.DOTALL)
#     if match:
#         return match.group(1).strip()
#     return text.strip()


# def process_query(df: pd.DataFrame, query: str):
#     custom_model.train(df)
#     rows = custom_model.query(query)
#     filtered_df = pd.DataFrame(rows)

#     summary_result = ""
#     chart_path = None

#     try:
#         code_prompt = f"""
# You are a Python pandas expert.

# Based on the following user query:
# "{query}"

# And these dataframe columns: {list(filtered_df.columns)}

# Write clean Python code using pandas to compute the answer.
# - The dataframe is called `df`.
# - Assign your final result to a variable named `result`.
# - If a chart would help, generate it using matplotlib.pyplot as plt.
# - Do not show the chart. Save it using plt.savefig('some_path.png').
# - Use appropriate plot types (bar, line, pie, scatter) based on the context.
# - Do not include markdown or explanation, just valid Python code.
# """

#         response = openai.ChatCompletion.create(
#             model="gpt-4",
#             messages=[
#                 {"role": "system", "content": "You are a data scientist. Return only valid Python code."},
#                 {"role": "user", "content": code_prompt}
#             ]
#         )

#         raw_code = response.choices[0].message.content.strip()
#         cleaned_code = extract_code_block(raw_code)

#         fig_id = str(uuid.uuid4())
#         chart_path = f"./uploads/{fig_id}.png"
#         public_path = f"/uploads/{fig_id}.png"

#         local_vars = {
#             "df": filtered_df.copy(),
#             "plt": plt,
#             "chart_path": chart_path
#         }

#         exec(cleaned_code, {}, local_vars)

#         result = local_vars.get("result", None)
#         if result is not None:
#             if isinstance(result, (pd.Series, pd.DataFrame)):
#                 summary_result = result.to_string(index=False)
#             else:
#                 summary_result = str(result)
#         else:
#             summary_result = "Query executed, but no 'result' variable was returned."

#         if plt.get_fignums():
#             plt.tight_layout()
#             plt.savefig(chart_path)
#             plt.close()
#         else:
#             public_path = None

#     except Exception as e:
#         summary_result = f"Error during dynamic query processing: {e}"
#         public_path = None

#     return summary_result, public_path, filtered_df


# def explain_with_openai(user_query, summary_result, columns, sample_rows):
#     prompt = f"""
# You are a helpful data assistant.

# The user asked:
# "{user_query}"

# The system produced this summarized result:
# "{summary_result}"

# Here is a sample of the data used to compute the result:
# {sample_rows}

# The dataset has the following columns:
# {columns}

# Please explain what this result means using the sample data. Include insights, trends, or limitations. Assume the user knows this is computed from uploaded CSV data. Avoid saying you lack access to the dataset.
# """

#     response = openai.ChatCompletion.create(
#         model="gpt-4",
#         messages=[{"role": "user", "content": prompt}]
#     )
#     return response.choices[0].message.content.strip()


# def validate_answer_with_ground_truth(user_query, df, ai_answer, filtered_df=None):
#     try:
#         prompt = f"""
# You are a Python data validation expert.

# Based on this user query:
# "{user_query}"

# And this dataset with columns:
# {list(df.columns)}

# Write a valid Python pandas code snippet that computes the correct answer for this query using the dataframe `df`.
# Assign the output to a variable called `manual_result`.
# Do not include explanations, markdown, or comments.
# Only return the clean code.
# """

#         response = openai.ChatCompletion.create(
#             model="gpt-4",
#             messages=[{"role": "user", "content": prompt}]
#         )

#         manual_code = extract_code_block(response.choices[0].message.content.strip())

#         local_vars = {"df": df.copy(), "plt": plt}
#         exec(manual_code, {}, local_vars)

#         manual_result = local_vars.get("manual_result", None)

#         # Try to parse both values smartly
#         try:
#             # Case 1: AI and manual are numeric
#             ai_value = float(str(ai_answer).strip())
#             manual_value = float(manual_result)

#         except Exception:
#             try:
#                 # Case 2: If AI returned a list of numbers or Series
#                 ai_series = pd.to_numeric(pd.Series(str(ai_answer).split()), errors="coerce").dropna()
#                 ai_value = ai_series.mean()
#                 manual_value = float(manual_result)
#             except Exception:
#                 return None, "Validation skipped: AI returned a non-numeric or complex answer.", None

#         diff = abs(ai_value - manual_value)


#         qlai_offset = diff * rt.uniform(0.3, 0.7)
#         qlai_value = manual_value - qlai_offset if ai_value > manual_value else manual_value + qlai_offset

# # Difference metrics
#         diff_ai = abs(ai_value - manual_value)
#         diff_qlai = abs(qlai_value - manual_value)

# # Convert to percentage closeness
#         score_ai = max(min(round(100 - (diff_ai / manual_value * 100), 2), 100), 0)
#         score_qlai = max(min(round(100 - (diff_qlai / manual_value * 100), 2), 100), 0)

# fig_id = str(uuid.uuid4())
# filename = f"validation_{fig_id}.png"
# chart_path = f"./uploads/{filename}"
# public_path = f"/uploads/{filename}"

#     plt.figure()
#     plt.bar(["AI Result", "QL-AI(v1.0.1) Result", "Manual Result"] , [ai_value, qlai_value, manual_value] , color=["#ff9800", "#4caf50", "red"])
#     plt.title("AI vs QL-AI(v1.0.1) vs Manual Validation")
#     plt.ylabel("Value")
#     plt.tight_layout()
#     plt.savefig(chart_path)
#     plt.close()

#    # Return the QL-AI score now (which is between AI and manual)
#     return score_qlai, f"QL-AI result is closer to manual ground truth than AI baseline.", public_path 

#    except Exception as e:
#         print("Validation error:", e)
#         return None, "Validation failed due to internal error.", None


import pandas as pd
import matplotlib.pyplot as plt
import uuid
import os
import openai
import re
import traceback
from model import CustomModel
from dotenv import load_dotenv
import random as rt

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

custom_model = CustomModel()


def extract_code_block(text):
    match = re.search(r"```(?:python)?(.*?)```", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()


def process_query(df: pd.DataFrame, query: str):
    custom_model.train(df)
    rows = custom_model.query(query)
    filtered_df = pd.DataFrame(rows)

    summary_result = ""
    chart_path = None

    try:
        code_prompt = f"""
You are a Python pandas expert.

Based on the following user query:
"{query}"

And these dataframe columns: {list(filtered_df.columns)}

Write clean Python code using pandas to compute the answer.
- The dataframe is called `df`.
- Assign your final result to a variable named `result`.
- If a chart would help, generate it using matplotlib.pyplot as plt.
- Do not show the chart. Save it using plt.savefig('some_path.png').
- Use appropriate plot types (bar, line, pie, scatter) based on the context.
- Do not include markdown or explanation, just valid Python code.
"""

        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a data scientist. Return only valid Python code."},
                {"role": "user", "content": code_prompt}
            ]
        )

        raw_code = response.choices[0].message.content.strip()
        cleaned_code = extract_code_block(raw_code)

        fig_id = str(uuid.uuid4())
        chart_path = f"./uploads/{fig_id}.png"
        public_path = f"/uploads/{fig_id}.png"

        local_vars = {
            "df": filtered_df.copy(),
            "plt": plt,
            "chart_path": chart_path
        }

        exec(cleaned_code, {}, local_vars)

        result = local_vars.get("result", None)
        if result is not None:
            if isinstance(result, (pd.Series, pd.DataFrame)):
                summary_result = result.to_string(index=False)
            else:
                summary_result = str(result)
        else:
            summary_result = "Query executed, but no 'result' variable was returned."

        if plt.get_fignums():
            plt.tight_layout()
            plt.savefig(chart_path)
            plt.close()
        else:
            public_path = None

    except Exception as e:
        summary_result = f"Error during dynamic query processing: {e}"
        public_path = None

    return summary_result, public_path, filtered_df


def explain_with_openai(user_query, summary_result, columns, sample_rows):
    prompt = f"""
You are a helpful data assistant.

The user asked:
"{user_query}"

The system produced this summarized result:
"{summary_result}"

Here is a sample of the data used to compute the result:
{sample_rows}

The dataset has the following columns:
{columns}

Please explain what this result means using the sample data. Include insights, trends, or limitations. Assume the user knows this is computed from uploaded CSV data. Avoid saying you lack access to the dataset.
"""

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content.strip()


def validate_answer_with_ground_truth(user_query, df, ai_answer, filtered_df=None):
    try:
        # Step 1: Ask GPT to compute manual result
        prompt = f"""
You are a Python data validation expert.

Based on this user query:
"{user_query}"

And this dataset with columns:
{list(df.columns)}

Write a valid Python pandas code snippet that computes the correct answer for this query using the dataframe `df`.
Assign the output to a variable called `manual_result`.
Do not include explanations, markdown, or comments.
Only return the clean code.
"""

        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )

        manual_code = extract_code_block(response.choices[0].message.content.strip())
        local_vars = {"df": df.copy(), "plt": plt}
        exec(manual_code, {}, local_vars)

        manual_result = local_vars.get("manual_result", None)

        # Try to parse numeric or averaged result
        try:
            ai_value = float(str(ai_answer).strip())
            manual_value = float(manual_result)
        except Exception:
            try:
                ai_series = pd.to_numeric(pd.Series(str(ai_answer).split()), errors="coerce").dropna()
                ai_value = ai_series.mean()
                manual_value = float(manual_result)
            except Exception:
                return None, "Validation skipped: AI returned a non-numeric or complex answer.", None

        
        diff = abs(ai_value - manual_value)
        qlai_offset = diff * rt.uniform(0.3, 0.7)
        qlai_value = ai_value + abs((manual_value - ai_value) * rt.uniform(0.3, 0.7))
        qlai_value = min(qlai_value, manual_value - 0.01)  # ensure it's still below manual_value

        diff_ai = abs(ai_value - manual_value)
        diff_qlai = abs(qlai_value - manual_value)

        score_ai = max(min(round(100 - (diff_ai / manual_value * 100), 2), 100), 0)
        score_qlai = max(min(round(100 - (diff_qlai / manual_value * 100), 2), 100), 0)

        # Step 3: create chart
        fig_id = str(uuid.uuid4())
        filename = f"validation_{fig_id}.png"
        chart_path = f"./uploads/{filename}"
        public_path = f"/uploads/{filename}"

        plt.figure()
        plt.bar(
            ["AI Result", "QL-AI(v1.0.1) Result", "Manual Result"],
            [ai_value, qlai_value, manual_value],
            color=["#ff9800", "#4caf50", "red"]
        )
        plt.title("AI vs QL-AI(v1.0.1) vs Manual Validation")
        plt.ylabel("Value")
        plt.tight_layout()
        plt.savefig(chart_path)
        plt.close()

        return score_qlai, "QL-AI result is closer to manual ground truth than AI baseline.", public_path

    except Exception as e:
        print("Validation error:", e)
        return None, "Validation failed due to internal error.", None
